{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedb4ed6",
   "metadata": {},
   "source": [
    "# Dice Cube Tracking with SAM 2 Video Predictor\n",
    "\n",
    "This notebook tracks and segments **green, red, and blue dice cubes** across all demonstration videos.\n",
    "\n",
    "**Pipeline:**\n",
    "1. **Grounding DINO** - Detect dice cubes by color on the first frame\n",
    "2. **SAM 2 Video Predictor** - Track and segment dice throughout the video\n",
    "3. **Workspace Filtering** - Only track objects on the tabletop (ignore background)\n",
    "\n",
    "**Color-coded tracking:**\n",
    "- ðŸŸ¢ Green dice\n",
    "- ðŸ”´ Red dice  \n",
    "- ðŸ”µ Blue dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8c196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement segment-anything-2 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for segment-anything-2\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Install Dependencies\n",
    "# ============================================================\n",
    "# Note: SAM2 must be installed from GitHub (not available on PyPI)\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124 -q\n",
    "%pip install transformers accelerate -q\n",
    "%pip install git+https://github.com/facebookresearch/segment-anything-2.git -q\n",
    "%pip install opencv-python supervision -q\n",
    "%pip install matplotlib numpy Pillow tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Import Libraries\n",
    "# ============================================================\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "import shutil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "VIDEO_DIR = \"./demonstrations\"\n",
    "OUTPUT_DIR = \"./output_tracking\"\n",
    "FRAMES_DIR = os.path.join(OUTPUT_DIR, \"frames\")  # Temporary frames for SAM2 video\n",
    "OUTPUT_VIDEOS_DIR = os.path.join(OUTPUT_DIR, \"videos\")\n",
    "TRACKING_DATA_DIR = os.path.join(OUTPUT_DIR, \"tracking_data\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FRAMES_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_VIDEOS_DIR, exist_ok=True)\n",
    "os.makedirs(TRACKING_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Gather all video files\n",
    "video_files = sorted([f for f in os.listdir(VIDEO_DIR) if f.endswith(('.mp4', '.avi', '.mov'))])\n",
    "print(f\"Found {len(video_files)} videos:\")\n",
    "for v in video_files:\n",
    "    print(f\"  - {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Grounding DINO for Initial Detection\n",
    "# ============================================================\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "GDINO_MODEL_ID = \"IDEA-Research/grounding-dino-base\"\n",
    "gdino_processor = AutoProcessor.from_pretrained(GDINO_MODEL_ID)\n",
    "gdino_model = AutoModelForZeroShotObjectDetection.from_pretrained(GDINO_MODEL_ID).to(DEVICE)\n",
    "gdino_model.eval()\n",
    "print(\"âœ“ Grounding DINO loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6296e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load SAM 2 for Video Segmentation & Tracking\n",
    "# ============================================================\n",
    "from sam2.build_sam import build_sam2_video_predictor, build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "# SAM2 checkpoint (you already have this)\n",
    "SAM2_CHECKPOINT = \"sam2_hiera_large.pt\"\n",
    "SAM2_CONFIG = \"sam2_hiera_l.yaml\"\n",
    "\n",
    "# Download if not present\n",
    "if not os.path.exists(SAM2_CHECKPOINT):\n",
    "    print(\"Downloading SAM2 checkpoint...\")\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\",\n",
    "        SAM2_CHECKPOINT\n",
    "    )\n",
    "\n",
    "# Build video predictor for tracking\n",
    "sam2_video_predictor = build_sam2_video_predictor(SAM2_CONFIG, SAM2_CHECKPOINT, device=DEVICE)\n",
    "\n",
    "# Build image predictor for initial segmentation\n",
    "sam2_model = build_sam2(SAM2_CONFIG, SAM2_CHECKPOINT, device=DEVICE)\n",
    "sam2_image_predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "print(\"SAM 2 Video Predictor loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f55cb8",
   "metadata": {},
   "source": [
    "## Detection & Filtering Functions\n",
    "\n",
    "These functions handle:\n",
    "1. **Dice detection** using Grounding DINO with color-specific prompts\n",
    "2. **Tabletop/workspace filtering** to ignore objects outside the work area\n",
    "3. **Color classification** to assign correct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Workspace/Tabletop Detection\n",
    "# ============================================================\n",
    "\n",
    "def detect_tabletop_region(image, method=\"color\"):\n",
    "    \"\"\"\n",
    "    Detect the tabletop/workspace region to filter out background objects.\n",
    "    \n",
    "    Methods:\n",
    "    - \"color\": Detect table by dominant color (works for colored tables)\n",
    "    - \"lower_half\": Simple heuristic - table is typically in lower portion\n",
    "    - \"full\": No filtering, use entire frame\n",
    "    \n",
    "    Returns: mask where True = workspace area\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    if method == \"lower_half\":\n",
    "        # Simple heuristic: table is in lower 70% of frame\n",
    "        mask = np.zeros((h, w), dtype=bool)\n",
    "        mask[int(h * 0.2):, :] = True\n",
    "        return mask\n",
    "    \n",
    "    elif method == \"color\":\n",
    "        # Detect table surface by color - typically a solid color\n",
    "        # Convert to HSV for better color segmentation\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Detect dominant non-white/non-black regions in lower half\n",
    "        lower_region = hsv[int(h * 0.5):, :]\n",
    "        \n",
    "        # Calculate histogram of hue values\n",
    "        hist = cv2.calcHist([lower_region], [0], None, [180], [0, 180])\n",
    "        dominant_hue = np.argmax(hist)\n",
    "        \n",
    "        # Create mask for table color (with tolerance)\n",
    "        lower_bound = np.array([max(0, dominant_hue - 15), 30, 50])\n",
    "        upper_bound = np.array([min(180, dominant_hue + 15), 255, 255])\n",
    "        color_mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "        \n",
    "        # Combine with spatial prior (lower portion more likely to be table)\n",
    "        spatial_weight = np.linspace(0.3, 1.0, h).reshape(-1, 1)\n",
    "        spatial_mask = np.tile(spatial_weight, (1, w))\n",
    "        \n",
    "        combined = (color_mask > 0).astype(float) * spatial_mask\n",
    "        mask = combined > 0.5\n",
    "        \n",
    "        # Clean up with morphology\n",
    "        kernel = np.ones((20, 20), np.uint8)\n",
    "        mask = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        return mask.astype(bool)\n",
    "    \n",
    "    else:  # \"full\"\n",
    "        return np.ones((h, w), dtype=bool)\n",
    "\n",
    "\n",
    "def is_in_workspace(box, workspace_mask, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Check if a detected object is within the workspace region.\n",
    "    \n",
    "    Args:\n",
    "        box: [x1, y1, x2, y2] bounding box\n",
    "        workspace_mask: Boolean mask of workspace region\n",
    "        threshold: Minimum overlap ratio required\n",
    "    \n",
    "    Returns: True if object is in workspace\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(workspace_mask.shape[1], x2), min(workspace_mask.shape[0], y2)\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return False\n",
    "    \n",
    "    box_region = workspace_mask[y1:y2, x1:x2]\n",
    "    overlap_ratio = np.mean(box_region)\n",
    "    \n",
    "    return overlap_ratio >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dice Detection with Grounding DINO\n",
    "# ============================================================\n",
    "\n",
    "def normalize_dice_label(raw_label):\n",
    "    \"\"\"\n",
    "    Normalize Grounding DINO labels to canonical dice colors.\n",
    "    \"\"\"\n",
    "    raw = raw_label.lower().strip()\n",
    "    \n",
    "    color_order = [\"green\", \"red\", \"blue\"]\n",
    "    for color in color_order:\n",
    "        if color in raw:\n",
    "            return f\"{color}_dice\"\n",
    "    \n",
    "    if \"dice\" in raw or \"cube\" in raw:\n",
    "        return \"unknown_dice\"\n",
    "    \n",
    "    return raw_label\n",
    "\n",
    "\n",
    "def classify_dice_by_color(image, box):\n",
    "    \"\"\"\n",
    "    Classify a dice by analyzing the dominant color in its bounding box.\n",
    "    More reliable than relying solely on Grounding DINO labels.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    h, w = image.shape[:2]\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(w, x2), min(h, y2)\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return \"unknown_dice\"\n",
    "    \n",
    "    roi = image[y1:y2, x1:x2]\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    # Define color ranges in HSV\n",
    "    color_ranges = {\n",
    "        \"red_dice\": [\n",
    "            (np.array([0, 100, 100]), np.array([10, 255, 255])),      # Red lower\n",
    "            (np.array([160, 100, 100]), np.array([180, 255, 255]))    # Red upper\n",
    "        ],\n",
    "        \"green_dice\": [\n",
    "            (np.array([35, 80, 80]), np.array([85, 255, 255]))        # Green\n",
    "        ],\n",
    "        \"blue_dice\": [\n",
    "            (np.array([90, 80, 80]), np.array([130, 255, 255]))       # Blue\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    color_scores = {}\n",
    "    for color_name, ranges in color_ranges.items():\n",
    "        total_mask = np.zeros(hsv.shape[:2], dtype=np.uint8)\n",
    "        for lower, upper in ranges:\n",
    "            mask = cv2.inRange(hsv, lower, upper)\n",
    "            total_mask = cv2.bitwise_or(total_mask, mask)\n",
    "        color_scores[color_name] = np.sum(total_mask) / (total_mask.size * 255)\n",
    "    \n",
    "    # Return color with highest score if above threshold\n",
    "    best_color = max(color_scores, key=color_scores.get)\n",
    "    if color_scores[best_color] > 0.15:  # At least 15% of pixels match\n",
    "        return best_color\n",
    "    \n",
    "    return \"unknown_dice\"\n",
    "\n",
    "\n",
    "def detect_colored_dice(image, workspace_mask=None, box_threshold=0.25, text_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detect all colored dice in an image using Grounding DINO.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or numpy array\n",
    "        workspace_mask: Optional mask to filter detections\n",
    "        box_threshold: Confidence threshold for boxes\n",
    "        text_threshold: Confidence threshold for text matching\n",
    "    \n",
    "    Returns: List of dicts with {box, label, score, color}\n",
    "    \"\"\"\n",
    "    if isinstance(image, np.ndarray):\n",
    "        pil_image = Image.fromarray(image)\n",
    "        np_image = image\n",
    "    else:\n",
    "        pil_image = image\n",
    "        np_image = np.array(image)\n",
    "    \n",
    "    # Text prompt for dice detection\n",
    "    text_prompt = \"green dice . red dice . blue dice . green cube . red cube . blue cube .\"\n",
    "    \n",
    "    inputs = gdino_processor(\n",
    "        images=pil_image,\n",
    "        text=text_prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = gdino_model(**inputs)\n",
    "    \n",
    "    results = gdino_processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        text_threshold=text_threshold,\n",
    "        target_sizes=[pil_image.size[::-1]]\n",
    "    )[0]\n",
    "    \n",
    "    boxes = results[\"boxes\"].cpu().numpy()\n",
    "    scores = results[\"scores\"].cpu().numpy()\n",
    "    labels = results[\"labels\"]\n",
    "    \n",
    "    # Filter by confidence\n",
    "    mask = scores >= box_threshold\n",
    "    boxes = boxes[mask]\n",
    "    scores = scores[mask]\n",
    "    labels = [labels[i] for i in range(len(labels)) if mask[i]]\n",
    "    \n",
    "    detections = []\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        # Filter by workspace if provided\n",
    "        if workspace_mask is not None and not is_in_workspace(box, workspace_mask):\n",
    "            continue\n",
    "        \n",
    "        # Classify color based on actual pixel values\n",
    "        color = classify_dice_by_color(np_image, box)\n",
    "        \n",
    "        detections.append({\n",
    "            \"box\": box,\n",
    "            \"score\": float(score),\n",
    "            \"label\": normalize_dice_label(label),\n",
    "            \"color\": color\n",
    "        })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "def apply_nms(detections, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Apply non-maximum suppression to remove duplicate detections.\n",
    "    \"\"\"\n",
    "    if len(detections) == 0:\n",
    "        return detections\n",
    "    \n",
    "    boxes = np.array([d[\"box\"] for d in detections])\n",
    "    scores = np.array([d[\"score\"] for d in detections])\n",
    "    \n",
    "    # Sort by score\n",
    "    order = scores.argsort()[::-1]\n",
    "    \n",
    "    keep = []\n",
    "    while len(order) > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        \n",
    "        if len(order) == 1:\n",
    "            break\n",
    "        \n",
    "        # Compute IoU with remaining boxes\n",
    "        xx1 = np.maximum(boxes[i, 0], boxes[order[1:], 0])\n",
    "        yy1 = np.maximum(boxes[i, 1], boxes[order[1:], 1])\n",
    "        xx2 = np.minimum(boxes[i, 2], boxes[order[1:], 2])\n",
    "        yy2 = np.minimum(boxes[i, 3], boxes[order[1:], 3])\n",
    "        \n",
    "        inter = np.maximum(0, xx2 - xx1) * np.maximum(0, yy2 - yy1)\n",
    "        area_i = (boxes[i, 2] - boxes[i, 0]) * (boxes[i, 3] - boxes[i, 1])\n",
    "        areas = (boxes[order[1:], 2] - boxes[order[1:], 0]) * (boxes[order[1:], 3] - boxes[order[1:], 1])\n",
    "        iou = inter / (area_i + areas - inter + 1e-6)\n",
    "        \n",
    "        remaining = np.where(iou < iou_threshold)[0] + 1\n",
    "        order = order[remaining]\n",
    "    \n",
    "    return [detections[i] for i in keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0b294",
   "metadata": {},
   "source": [
    "## SAM 2 Video Tracking Pipeline\n",
    "\n",
    "This section implements the core tracking logic:\n",
    "1. Extract frames from video\n",
    "2. Initialize SAM 2 with first-frame detections\n",
    "3. Propagate segmentation masks through all frames\n",
    "4. Generate output video with tracked dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6250c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Video Frame Extraction for SAM2\n",
    "# ============================================================\n",
    "\n",
    "def extract_frames_for_sam2(video_path, output_dir, sample_rate=1):\n",
    "    \"\"\"\n",
    "    Extract frames from video for SAM2 video predictor.\n",
    "    SAM2 requires JPEG frames in a directory.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        output_dir: Directory to save frames\n",
    "        sample_rate: Extract every Nth frame (1 = all frames)\n",
    "    \n",
    "    Returns: (frame_paths, fps, total_frames, frame_indices)\n",
    "    \"\"\"\n",
    "    # Clear output directory\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    frame_paths = []\n",
    "    frame_indices = []\n",
    "    frame_idx = 0\n",
    "    saved_idx = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_idx % sample_rate == 0:\n",
    "            # SAM2 expects sequential naming\n",
    "            frame_path = os.path.join(output_dir, f\"{saved_idx:06d}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            frame_paths.append(frame_path)\n",
    "            frame_indices.append(frame_idx)\n",
    "            saved_idx += 1\n",
    "        \n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    return frame_paths, fps, total_frames, frame_indices, (width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAM2 Video Tracking Core\n",
    "# ============================================================\n",
    "\n",
    "# Color mapping for visualization\n",
    "DICE_COLORS = {\n",
    "    \"green_dice\": (0, 255, 0),      # Green\n",
    "    \"red_dice\": (255, 0, 0),        # Red  \n",
    "    \"blue_dice\": (0, 0, 255),       # Blue\n",
    "    \"unknown_dice\": (255, 255, 0),  # Yellow\n",
    "}\n",
    "\n",
    "\n",
    "def track_dice_in_video(video_path, sample_rate=2, workspace_method=\"lower_half\"):\n",
    "    \"\"\"\n",
    "    Track all colored dice cubes in a video using SAM2.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        sample_rate: Process every Nth frame (lower = more accurate, slower)\n",
    "        workspace_method: Method to detect tabletop (\"lower_half\", \"color\", \"full\")\n",
    "    \n",
    "    Returns: Dictionary with tracking results\n",
    "    \"\"\"\n",
    "    video_name = Path(video_path).stem\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {video_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create temporary frame directory for this video\n",
    "    video_frames_dir = os.path.join(FRAMES_DIR, video_name)\n",
    "    \n",
    "    # Step 1: Extract frames\n",
    "    print(\"Step 1: Extracting frames...\")\n",
    "    frame_paths, fps, total_frames, frame_indices, (width, height) = \\\n",
    "        extract_frames_for_sam2(video_path, video_frames_dir, sample_rate)\n",
    "    print(f\"  - Extracted {len(frame_paths)} frames (original: {total_frames} @ {fps:.1f} FPS)\")\n",
    "    \n",
    "    # Step 2: Detect dice on first frame\n",
    "    print(\"Step 2: Detecting dice on first frame...\")\n",
    "    first_frame = cv2.cvtColor(cv2.imread(frame_paths[0]), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect workspace/tabletop\n",
    "    workspace_mask = detect_tabletop_region(first_frame, method=workspace_method)\n",
    "    \n",
    "    # Detect dice\n",
    "    detections = detect_colored_dice(first_frame, workspace_mask, box_threshold=0.20)\n",
    "    detections = apply_nms(detections, iou_threshold=0.4)\n",
    "    \n",
    "    if len(detections) == 0:\n",
    "        print(\"  âš  No dice detected on first frame! Trying without workspace filter...\")\n",
    "        detections = detect_colored_dice(first_frame, None, box_threshold=0.15)\n",
    "        detections = apply_nms(detections, iou_threshold=0.4)\n",
    "    \n",
    "    print(f\"  - Found {len(detections)} dice:\")\n",
    "    for i, det in enumerate(detections):\n",
    "        print(f\"    [{i}] {det['color']} (conf: {det['score']:.2f})\")\n",
    "    \n",
    "    if len(detections) == 0:\n",
    "        print(\"  âœ— No dice found, skipping video\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Initialize SAM2 video predictor\n",
    "    print(\"Step 3: Initializing SAM2 video tracking...\")\n",
    "    \n",
    "    with torch.inference_mode(), torch.autocast(DEVICE, dtype=torch.bfloat16):\n",
    "        state = sam2_video_predictor.init_state(video_path=video_frames_dir)\n",
    "        \n",
    "        # Add each detected dice as a tracking object\n",
    "        object_ids = []\n",
    "        object_colors = {}\n",
    "        \n",
    "        for i, det in enumerate(detections):\n",
    "            obj_id = i + 1  # SAM2 uses 1-indexed object IDs\n",
    "            box = det[\"box\"]\n",
    "            \n",
    "            # Add object with bounding box prompt\n",
    "            _, out_obj_ids, out_mask_logits = sam2_video_predictor.add_new_points_or_box(\n",
    "                inference_state=state,\n",
    "                frame_idx=0,\n",
    "                obj_id=obj_id,\n",
    "                box=box\n",
    "            )\n",
    "            \n",
    "            object_ids.append(obj_id)\n",
    "            object_colors[obj_id] = det[\"color\"]\n",
    "        \n",
    "        print(f\"  - Initialized {len(object_ids)} objects for tracking\")\n",
    "        \n",
    "        # Step 4: Propagate through video\n",
    "        print(\"Step 4: Propagating masks through video...\")\n",
    "        \n",
    "        # Collect all frame masks\n",
    "        video_segments = {}  # {frame_idx: {obj_id: mask}}\n",
    "        \n",
    "        for frame_idx, obj_ids, mask_logits in sam2_video_predictor.propagate_in_video(state):\n",
    "            masks = (mask_logits > 0.0).cpu().numpy()\n",
    "            video_segments[frame_idx] = {}\n",
    "            \n",
    "            for i, obj_id in enumerate(obj_ids):\n",
    "                video_segments[frame_idx][obj_id] = masks[i, 0]  # [H, W] boolean mask\n",
    "        \n",
    "        print(f\"  - Tracked across {len(video_segments)} frames\")\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        \"video_name\": video_name,\n",
    "        \"video_path\": video_path,\n",
    "        \"fps\": fps,\n",
    "        \"total_frames\": total_frames,\n",
    "        \"processed_frames\": len(frame_paths),\n",
    "        \"sample_rate\": sample_rate,\n",
    "        \"frame_size\": (width, height),\n",
    "        \"frame_indices\": frame_indices,\n",
    "        \"objects\": {\n",
    "            obj_id: {\n",
    "                \"color\": object_colors[obj_id],\n",
    "                \"initial_box\": detections[obj_id - 1][\"box\"].tolist()\n",
    "            }\n",
    "            for obj_id in object_ids\n",
    "        },\n",
    "        \"segments\": video_segments,\n",
    "        \"frame_paths\": frame_paths\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization & Video Output\n",
    "# ============================================================\n",
    "\n",
    "def create_tracking_video(results, output_path):\n",
    "    \"\"\"\n",
    "    Create output video with segmentation masks overlaid.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    frame_paths = results[\"frame_paths\"]\n",
    "    segments = results[\"segments\"]\n",
    "    objects = results[\"objects\"]\n",
    "    fps = results[\"fps\"] / results[\"sample_rate\"]  # Adjust for sampled frames\n",
    "    \n",
    "    # Get frame size\n",
    "    first_frame = cv2.imread(frame_paths[0])\n",
    "    height, width = first_frame.shape[:2]\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"  Creating output video: {output_path}\")\n",
    "    \n",
    "    for frame_idx, frame_path in enumerate(tqdm(frame_paths, desc=\"  Rendering\")):\n",
    "        frame = cv2.imread(frame_path)\n",
    "        \n",
    "        if frame_idx in segments:\n",
    "            for obj_id, mask in segments[frame_idx].items():\n",
    "                if obj_id in objects:\n",
    "                    color_name = objects[obj_id][\"color\"]\n",
    "                    color_bgr = DICE_COLORS.get(color_name, (255, 255, 0))\n",
    "                    # Convert RGB to BGR for OpenCV\n",
    "                    color_bgr = (color_bgr[2], color_bgr[1], color_bgr[0])\n",
    "                    \n",
    "                    # Apply mask overlay\n",
    "                    mask_3ch = np.stack([mask] * 3, axis=-1)\n",
    "                    overlay = frame.copy()\n",
    "                    overlay[mask] = color_bgr\n",
    "                    frame = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
    "                    \n",
    "                    # Draw contour\n",
    "                    contours, _ = cv2.findContours(\n",
    "                        mask.astype(np.uint8), \n",
    "                        cv2.RETR_EXTERNAL, \n",
    "                        cv2.CHAIN_APPROX_SIMPLE\n",
    "                    )\n",
    "                    cv2.drawContours(frame, contours, -1, color_bgr, 2)\n",
    "                    \n",
    "                    # Add label\n",
    "                    if contours:\n",
    "                        M = cv2.moments(contours[0])\n",
    "                        if M[\"m00\"] > 0:\n",
    "                            cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                            label = color_name.replace(\"_\", \" \").title()\n",
    "                            cv2.putText(frame, label, (cx - 30, cy - 10),\n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        \n",
    "        out.write(frame)\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"  âœ“ Saved: {output_path}\")\n",
    "\n",
    "\n",
    "def visualize_first_frame_detections(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the initial detections on the first frame.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    frame_path = results[\"frame_paths\"][0]\n",
    "    frame = cv2.cvtColor(cv2.imread(frame_path), cv2.COLOR_BGR2RGB)\n",
    "    segments = results[\"segments\"].get(0, {})\n",
    "    objects = results[\"objects\"]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Original with boxes\n",
    "    ax1 = axes[0]\n",
    "    ax1.imshow(frame)\n",
    "    ax1.set_title(f\"Initial Detections - {results['video_name']}\")\n",
    "    \n",
    "    for obj_id, obj_info in objects.items():\n",
    "        box = obj_info[\"initial_box\"]\n",
    "        color = DICE_COLORS.get(obj_info[\"color\"], (255, 255, 0))\n",
    "        # Normalize to 0-1 for matplotlib\n",
    "        color_norm = tuple(c / 255 for c in color)\n",
    "        \n",
    "        rect = plt.Rectangle(\n",
    "            (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "            fill=False, edgecolor=color_norm, linewidth=2\n",
    "        )\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(box[0], box[1] - 5, obj_info[\"color\"].replace(\"_\", \" \"),\n",
    "                color=color_norm, fontsize=10, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # With segmentation masks\n",
    "    ax2 = axes[1]\n",
    "    mask_overlay = frame.copy().astype(float)\n",
    "    \n",
    "    for obj_id, mask in segments.items():\n",
    "        if obj_id in objects:\n",
    "            color = DICE_COLORS.get(objects[obj_id][\"color\"], (255, 255, 0))\n",
    "            for c in range(3):\n",
    "                mask_overlay[:, :, c] = np.where(\n",
    "                    mask, \n",
    "                    mask_overlay[:, :, c] * 0.5 + color[c] * 0.5,\n",
    "                    mask_overlay[:, :, c]\n",
    "                )\n",
    "    \n",
    "    ax2.imshow(mask_overlay.astype(np.uint8))\n",
    "    ax2.set_title(f\"Segmentation Masks - {results['video_name']}\")\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d59a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save Tracking Data\n",
    "# ============================================================\n",
    "\n",
    "def save_tracking_data(results, output_dir):\n",
    "    \"\"\"\n",
    "    Save tracking data (centroids, bounding boxes per frame) to JSON.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    video_name = results[\"video_name\"]\n",
    "    \n",
    "    # Extract centroid and bbox data per frame\n",
    "    tracking_data = {\n",
    "        \"video_name\": video_name,\n",
    "        \"fps\": results[\"fps\"],\n",
    "        \"total_frames\": results[\"total_frames\"],\n",
    "        \"sample_rate\": results[\"sample_rate\"],\n",
    "        \"objects\": {},\n",
    "        \"frames\": {}\n",
    "    }\n",
    "    \n",
    "    # Object info\n",
    "    for obj_id, obj_info in results[\"objects\"].items():\n",
    "        tracking_data[\"objects\"][str(obj_id)] = {\n",
    "            \"color\": obj_info[\"color\"]\n",
    "        }\n",
    "    \n",
    "    # Per-frame tracking\n",
    "    for frame_idx, masks in results[\"segments\"].items():\n",
    "        original_frame_idx = results[\"frame_indices\"][frame_idx]\n",
    "        tracking_data[\"frames\"][str(original_frame_idx)] = {}\n",
    "        \n",
    "        for obj_id, mask in masks.items():\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "            \n",
    "            # Get bounding box from mask\n",
    "            ys, xs = np.where(mask)\n",
    "            if len(xs) == 0:\n",
    "                continue\n",
    "            \n",
    "            bbox = [int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())]\n",
    "            centroid = [int(np.mean(xs)), int(np.mean(ys))]\n",
    "            area = int(np.sum(mask))\n",
    "            \n",
    "            tracking_data[\"frames\"][str(original_frame_idx)][str(obj_id)] = {\n",
    "                \"bbox\": bbox,\n",
    "                \"centroid\": centroid,\n",
    "                \"area\": area,\n",
    "                \"color\": results[\"objects\"][obj_id][\"color\"]\n",
    "            }\n",
    "    \n",
    "    # Save to JSON\n",
    "    output_path = os.path.join(output_dir, f\"{video_name}_tracking.json\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(tracking_data, f, indent=2)\n",
    "    \n",
    "    print(f\"  âœ“ Saved tracking data: {output_path}\")\n",
    "    return tracking_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1381d4",
   "metadata": {},
   "source": [
    "## Process All Demonstration Videos\n",
    "\n",
    "Run the tracking pipeline on all videos in the demonstrations folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb17e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Process All Videos\n",
    "# ============================================================\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_RATE = 2  # Process every 2nd frame (balance speed/accuracy)\n",
    "WORKSPACE_METHOD = \"lower_half\"  # Options: \"lower_half\", \"color\", \"full\"\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for video_file in video_files:\n",
    "    video_path = os.path.join(VIDEO_DIR, video_file)\n",
    "    video_name = Path(video_file).stem\n",
    "    \n",
    "    try:\n",
    "        # Track dice in video\n",
    "        results = track_dice_in_video(\n",
    "            video_path, \n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            workspace_method=WORKSPACE_METHOD\n",
    "        )\n",
    "        \n",
    "        if results is not None:\n",
    "            all_results[video_name] = results\n",
    "            \n",
    "            # Visualize first frame detections\n",
    "            vis_path = os.path.join(OUTPUT_DIR, f\"{video_name}_detections.png\")\n",
    "            visualize_first_frame_detections(results, save_path=vis_path)\n",
    "            \n",
    "            # Create output video with tracking\n",
    "            output_video_path = os.path.join(OUTPUT_VIDEOS_DIR, f\"{video_name}_tracked.mp4\")\n",
    "            create_tracking_video(results, output_video_path)\n",
    "            \n",
    "            # Save tracking data\n",
    "            save_tracking_data(results, TRACKING_DATA_DIR)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error processing {video_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Processing complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Videos processed: {len(all_results)}/{len(video_files)}\")\n",
    "print(f\"Output videos: {OUTPUT_VIDEOS_DIR}\")\n",
    "print(f\"Tracking data: {TRACKING_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37212af8",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "View a summary of all tracked dice across videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Summary Statistics\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRACKING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_data = []\n",
    "for video_name, results in all_results.items():\n",
    "    objects = results[\"objects\"]\n",
    "    \n",
    "    # Count by color\n",
    "    color_counts = {\"green_dice\": 0, \"red_dice\": 0, \"blue_dice\": 0, \"unknown_dice\": 0}\n",
    "    for obj_info in objects.values():\n",
    "        color = obj_info[\"color\"]\n",
    "        if color in color_counts:\n",
    "            color_counts[color] += 1\n",
    "    \n",
    "    summary_data.append({\n",
    "        \"video\": video_name,\n",
    "        \"green\": color_counts[\"green_dice\"],\n",
    "        \"red\": color_counts[\"red_dice\"],\n",
    "        \"blue\": color_counts[\"blue_dice\"],\n",
    "        \"total\": len(objects),\n",
    "        \"frames\": results[\"processed_frames\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{video_name}:\")\n",
    "    print(f\"  ðŸŸ¢ Green dice: {color_counts['green_dice']}\")\n",
    "    print(f\"  ðŸ”´ Red dice:   {color_counts['red_dice']}\")\n",
    "    print(f\"  ðŸ”µ Blue dice:  {color_counts['blue_dice']}\")\n",
    "    print(f\"  ðŸ“Š Total tracked: {len(objects)} objects over {results['processed_frames']} frames\")\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Video':<20} {'Green':>8} {'Red':>8} {'Blue':>8} {'Total':>8} {'Frames':>8}\")\n",
    "print(\"-\"*70)\n",
    "for row in summary_data:\n",
    "    print(f\"{row['video']:<20} {row['green']:>8} {row['red']:>8} {row['blue']:>8} {row['total']:>8} {row['frames']:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e62e6",
   "metadata": {},
   "source": [
    "## Single Video Test (Optional)\n",
    "\n",
    "Use this cell to test tracking on a single video before processing all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test on Single Video (run this first to verify setup)\n",
    "# ============================================================\n",
    "\n",
    "# Pick the first video for testing\n",
    "TEST_VIDEO = video_files[0] if video_files else None\n",
    "\n",
    "if TEST_VIDEO:\n",
    "    test_video_path = os.path.join(VIDEO_DIR, TEST_VIDEO)\n",
    "    \n",
    "    # Track with higher sample rate for faster testing\n",
    "    test_results = track_dice_in_video(\n",
    "        test_video_path,\n",
    "        sample_rate=5,  # Faster for testing\n",
    "        workspace_method=\"lower_half\"\n",
    "    )\n",
    "    \n",
    "    if test_results:\n",
    "        # Visualize\n",
    "        visualize_first_frame_detections(test_results)\n",
    "        \n",
    "        # Create short test video\n",
    "        test_output = os.path.join(OUTPUT_VIDEOS_DIR, f\"{Path(TEST_VIDEO).stem}_test.mp4\")\n",
    "        create_tracking_video(test_results, test_output)\n",
    "        \n",
    "        print(\"\\nâœ“ Test complete! Check the output above to verify dice detection.\")\n",
    "        print(\"  If dice are detected correctly, run the 'Process All Videos' cell.\")\n",
    "else:\n",
    "    print(\"No videos found in demonstrations folder!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
